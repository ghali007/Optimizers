{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Tensorflow Addons\nTensorFlow SIG Addons is a repository of community contributions that conform to well-established API patterns, but implement new functionality not available in core TensorFlow.","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.342975Z","iopub.execute_input":"2021-08-03T00:52:11.343359Z","iopub.status.idle":"2021-08-03T00:52:11.515780Z","shell.execute_reply.started":"2021-08-03T00:52:11.343329Z","shell.execute_reply":"2021-08-03T00:52:11.514647Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# default use of AdamW\nstep = tf.Variable(0, trainable=False)\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n    [10000, 15000], [1e-0, 1e-1, 1e-2])\n# lr and wd can be a function or a tensor\nlr = 1e-1 * schedule(step)\nwd = lambda: 1e-4 * schedule(step)\noptimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.517309Z","iopub.execute_input":"2021-08-03T00:52:11.517644Z","iopub.status.idle":"2021-08-03T00:52:11.571363Z","shell.execute_reply.started":"2021-08-03T00:52:11.517614Z","shell.execute_reply":"2021-08-03T00:52:11.570370Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# another way to use AdamW\nLr_type=[tf.keras.experimental.CosineDecay(1e-3,800,),\n         tf.keras.experimental.CosineDecayRestarts(0.001, 0, t_mul=2.0, m_mul=1.0, alpha=0.0),\n         tf.keras.experimental.NoisyLinearCosineDecay(0.01, 0.8, initial_variance=1.0, variance_decay=0.55,num_periods=0.5, alpha=0.0, beta=0.001)]\n\nopt = tfa.optimizers.AdamW(Lr_type[0], learning_rate=1e-4)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.573054Z","iopub.execute_input":"2021-08-03T00:52:11.573362Z","iopub.status.idle":"2021-08-03T00:52:11.579668Z","shell.execute_reply.started":"2021-08-03T00:52:11.573334Z","shell.execute_reply":"2021-08-03T00:52:11.578731Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# default use of RectifiedAdam\nopt = tfa.optimizers.RectifiedAdam(lr=1e-3,total_steps=10000,warmup_proportion=0.1,min_lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.581319Z","iopub.execute_input":"2021-08-03T00:52:11.581785Z","iopub.status.idle":"2021-08-03T00:52:11.595281Z","shell.execute_reply.started":"2021-08-03T00:52:11.581742Z","shell.execute_reply":"2021-08-03T00:52:11.594357Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# another way to use RectifiedAdam using Lookahead\n\nimport tensorflow_addons as tfa\nopt = tfa.optimizers.RectifiedAdam()\nhard_opt=tfa.optimizers.Lookahead(opt, sync_period=10)\n\n#also we can use all the opimizers of tensorflow with Lookahead\n\nopt = tf.keras.optimizers.SGD(lr=0.001)\nopt = tfa.optimizers.Lookahead(opt, sync_period=10)\n##############################################################\nopt = tf.keras.optimizers.Adam(lr=0.001)\nopt = tfa.optimizers.Lookahead(opt, sync_period=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.596536Z","iopub.execute_input":"2021-08-03T00:52:11.596990Z","iopub.status.idle":"2021-08-03T00:52:11.609028Z","shell.execute_reply.started":"2021-08-03T00:52:11.596951Z","shell.execute_reply":"2021-08-03T00:52:11.608174Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### SGDW\nIt computes the update step of tf.keras.optimizers.SGD and additionally decays the variable. Note that this is different from adding L2 regularization on the variables to the loss. Decoupling the weight decay from other hyperparameters (in particular the learning rate) simplifies hyperparameter search.\nfor further infos see [SGDW](https://arxiv.org/abs/1711.05101) ","metadata":{}},{"cell_type":"code","source":"step = tf.Variable(0, trainable=False)\nschedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n    [10000, 15000], [1e-0, 1e-1, 1e-2])\n# lr and wd can be a function or a tensor\nlr = 1e-1 * schedule(step)\nwd = lambda: 1e-4 * schedule(step)\n\noptimizer = tfa.optimizers.SGDW(learning_rate=lr, weight_decay=wd, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.610420Z","iopub.execute_input":"2021-08-03T00:52:11.610967Z","iopub.status.idle":"2021-08-03T00:52:11.658870Z","shell.execute_reply.started":"2021-08-03T00:52:11.610933Z","shell.execute_reply":"2021-08-03T00:52:11.658056Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### how to use MultiOptimizer\nEach optimizer will optimize only the weights associated with its paired layer. This can be used to implement discriminative layer training by assigning different learning rates to each optimizer layer pair. (tf.keras.optimizers.Optimizer, List[tf.keras.layers.Layer]) pairs are also supported. Please note that the layers must be instantiated before instantiating the optimizer.","metadata":{}},{"cell_type":"code","source":"#how to use MultiOptimizer\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(4,)),\n    tf.keras.layers.Dense(8),\n    tf.keras.layers.Dense(16),\n    tf.keras.layers.Dense(32),\n])\n\noptimizers = [\n    tf.keras.optimizers.Adam(learning_rate=1e-4),\n    tf.keras.optimizers.SGD(learning_rate=1e-2)\n]\noptimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\noptimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\nmodel.compile(optimizer=optimizer, loss=\"mse\")\n\n#note that model here is your model that you gonna use to solve your problem (classification for example)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.660134Z","iopub.execute_input":"2021-08-03T00:52:11.660628Z","iopub.status.idle":"2021-08-03T00:52:11.750872Z","shell.execute_reply.started":"2021-08-03T00:52:11.660591Z","shell.execute_reply":"2021-08-03T00:52:11.749783Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"[for more optimizers check this link](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers)","metadata":{}},{"cell_type":"markdown","source":"### how to change the optimizer during the training process","metadata":{}},{"cell_type":"code","source":"#SGDR tooked from this link: https://github.com/YeongHyeon/ResNet-with-SGDR-TF2/blob/master/function_sgdr.py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLR = 0.0008\nWEIGHT_DECAY = 0\nEPOCHS = 100\nWARMUP = 25\n\ndef get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5):\n    \"\"\"\n    Modified the get_cosine_schedule_with_warmup from huggingface for tenserflow\n    (https://huggingface.co/transformers/_modules/transformers/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return float(epoch) / float(max(1, num_warmup_steps)) * lr\n        progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here at each 25 epochs we change the optimizer, i believe that we the use of Stochastic Gradient Descent with Warm Restarts (SGDR), in the callbacks\n# if we fixe the new warm restart at every 25 epochs, meaning in parallel with the changement of the optimizer a new warm restart. \n#(this can help us to know wich optimizer performs better with the SGDR)\noptimizers = [tf.keras.optimizers.Adam(learning_rate=1e-4), \n              tf.keras.optimizers.SGD(learning_rate=1e-2), \n              tf.keras.optimizers.Adam(learning_rate=1e-6), \n              tf.keras.optimizers.SGD(learning_rate=1e-4)]\n\nopts=['Adam', 'SGD', 'Adam', 'SGD']\nlrs=[1e-4,1e-2,1e-6,1e-4]\nepochs = [25, 25, 25, 25]\nEPOCH_STEPS=250\n\nfor i in range(len(optimizers)):\n    print('Using optimizer: ' + opts[i] +' with lr: '+str(lrs[i])+', Epoch: ' + str(epochs[i]))\n    \n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.3, \n                                                               reduction=tf.keras.losses.Reduction.AUTO,\n                                                               name='categorical_crossentropy'), \n                  optimizer=optimizer[i], \n                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n    \n    train_history = model.fit_generator(\n            train_generator,\n            steps_per_epoch=EPOCH_STEPS,\n            epochs=epochs[i],\n            callbacks=[get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5)],\n            shuffle=True\n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TFlearn\nTFlearn is a modular and transparent deep learning library built on top of Tensorflow. It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.","metadata":{}},{"cell_type":"code","source":"#install tflearn\n!pip install tflearn\nimport tflearn","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.804767Z","iopub.status.idle":"2021-08-03T00:52:11.805236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.SGD (learning_rate=0.001, lr_decay=0.0, decay_step=100, staircase=False, use_locking=False, name='SGD')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.806785Z","iopub.status.idle":"2021-08-03T00:52:11.807446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.Adam (learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.808783Z","iopub.status.idle":"2021-08-03T00:52:11.809487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.AdaGrad (learning_rate=0.001, initial_accumulator_value=0.1, use_locking=False, name='AdaGrad')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.810859Z","iopub.status.idle":"2021-08-03T00:52:11.811525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.Ftrl (learning_rate=3.0, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=0.0, l2_regularization_strength=0.0, use_locking=False, name='Ftrl')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.812860Z","iopub.status.idle":"2021-08-03T00:52:11.813521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.ProximalAdaGrad (learning_rate=0.001, initial_accumulator_value=0.1, use_locking=False, name='AdaGrad')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.815302Z","iopub.status.idle":"2021-08-03T00:52:11.815762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt=tflearn.optimizers.Nesterov (learning_rate=0.001, momentum=0.9, lr_decay=0.0, decay_step=100, staircase=False, use_locking=False, name='Nesterov')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T00:52:11.816623Z","iopub.status.idle":"2021-08-03T00:52:11.817106Z"},"trusted":true},"execution_count":null,"outputs":[]}]}